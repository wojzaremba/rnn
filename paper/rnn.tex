
% Use the following line  only  if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}
\usepackage{comment}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2014}



\begin{document} 

\twocolumn[
\icmltitle{Recurrent neural network as a language model - technical report}

\icmlauthor{Wojciech Zaremba}{woj.zaremba@gmail.com}
\icmladdress{New York University}
%\icmlauthor{Rob Fergus}{fergus@cs.nyu.edu}
%\icmladdress{New York University}
%\vskip -0.12in
%\icmladdress{Facebook AI Group}

\icmlkeywords{natual language processing, recurrent neural networks, language model}

\vskip 0.3in
]

\begin{abstract} 
  Recurrent neural networks (RNN) offer a powerful framework to learn any arbitrary dependency.
  They are expressive as a finite memory Turning machine. However, their 
  training is difficult and computationally expensive.

  This technical note focuses on training RNNs for modeling language
  at the character level.  We provide set of pragmatic recommendations
  about how to train a simple one layer RNN for this task.
  Moreover, we provide CPU and GPU Theano
  \cite{bergstra+al:2010-scipy} code which reproduces results close to
  state-of-the-art on the Penn Treebank Corpus.
\end{abstract} 

\section{Introduction}
Neural networks (NN) are stacked linear transformations alternated
with non-linearities.  Currently, state-of-the-art results in many
computer vision tasks are achieved with feed-forward neural networks
of this type.
In feed-forward networks, computation flows in one direction from the input
layer to the output layer(s).  Recurrent neural networks (RNN) contain
connections between instances of feed forward networks shifted in
time. Such connections allow them to maintain {\em memory}, and perform
prediction dependent on a history. Based on current advances in
computer vision thanks to feed-forward networks, we are optimistic that
models heavily utilizing RNNs can superior results to the current
state-of-the-art on NLP tasks.  Moreover, we believe that they
might be crucial for further advances in computer vision (attention
based models, and video prediction).


A common setting for RNNs is the prediction of the next element in a
sequence. The input is a single element of a sequence, and a previous
state. The network attempts at every stage to predict next element of
sequence.  We examine these models for language modelling and for
simplicity, we constrain ourselves to a character level language
model.



The typical training procedure for RNNs is stochastic gradient descent
(SGD). However, it is difficult to obtain effective RNN models by
applying unconstrained SGD.  Recurrency brings much higher expressive
power compared to feed-forward networks, but also makes them more
difficult to train. There are several well-known issues: (1) vanishing
gradient; (2) exploding gradient and (3) short memory. We address
exploding gradient issue by clipping gradients, but don't tackle the 
remaining problems.


We proceed by presenting related work (Section \ref{sec:related
  work}). Next, we describe our framework (Section
\ref{sec:framework}), and finally we present experimental results
(Section \ref{sec:experiments}).  Code reproducing
experiments (and train any arbitrary RNN on CPU or GPU) is available
online\footnote{\url{https://github.com/wojzaremba/rnn}}.

\section{Related work}\label{sec:related work}
There has been extensive interest in different flavours of neural networks 
with recurrent connections \cite{hopfield1982neural, hinton2006fast}. These
approaches consider recurrency not to account for time dependency, but
for internal data dependency (e.g. correlation between pixel values). 


In this note, we are mainly interested in RNNs which aim to predict 
a temporal sequence. \cite{mikolov2012statistical} 
\cite{sutskever2013training} consider
training of such networks at the character and word level. Moreover,
they analyze how best optimize such models (e.g.~with Hessian-free method, or 
by clipping gradients).


\cite{graves2013generating} shows how the memory of the model may be
extended by using Long-Short-Term-Memory units (LSTSs). Some evidence
is shown that LSTMs prevent gradients from vanishing.

\section{Framework}\label{sec:framework}
Our code is build in Theano. This Python framework permits the
definition of symbolic expressions, and their automatic
differentiation. Moreover, it compiles code to fast CPU or GPU
executable versions. We now present the setup for our best model.


We train a simple RNN as shown in Figure \ref{fig:schema}. Our best
model has 600 hidden units.  We initialize all weights randomly from a
zero-mean Gausssian with variance $0.001$, and all biases are set to
$0$.  When switching between different instances of text, the hidden
units should be reset to all ones (this works better than all zeros).
We train the model with SGD on minibatches of size 10, and we unroll RNN
for 10 steps. We don't use any momentum (for such a small model it
makes little difference). We clip gradients above an $\ell_2$ norm of
$15$, to have a norm of $15$. The 
initial learning rate is $0.1$. We decrease it by factor of $2$ every
time when perplexity on a validation set increases (we measure
perplexity in every epoch). It takes around $5-8$ epochs to decrease
the learning rate. A single epoch takes around $20$ minutes to compute. If perplexity
increases for 2 epochs then we finish training (early stopping
criteria).



\begin{figure}
  \subfigure[This figure presents the information flow in a recurrent network
  based on the simplest possible neural network.  Experiments in this
  paper have been perfomed on such networks, where non-linearity is a
  sigmoid, and classifier is a softmax. We consider models with hidden
  layer of the size $200, 600, 1000$, and input is restricted to ASCII
  characters.  The input can be represented as an indicator vector having
  256 dimensions, however in practise is better to store as an uint8.]{
    \includegraphics[width=\linewidth]{img/schema.png}
  }\label{fig:schema}
\end{figure}

The final layer is a softmax. This classifier gives probabilities of
ocurrence for next character. During sequence generation , we sample
from this probability.


\section{Experiments}\label{sec:experiments}
First, we use synthetic data to show experiments that verify if our model has any memory. 
Then we trained it Penn Treebank Corpus\footnote{\url{http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz}}.  


\subsection{Synthetic data}
Our first experiment demonstrates the memory of the model. A network is
trained on sequences ``aabbccddaabbccdd$\dots$'', starting from
any arbitrary position. In order to give a proper prediction of
the next character, the network has to remember what is the current
position, i.e~ if it is a first ``a'', or a second ``a''. Starting from any
random position (with clean hidden state), for first two predictions the
network is uncertain about next character. However, after two
predictions, all subsequent predictions are deterministic and the network learns to
predict them correctly. Very short networks can obtain a perplexity close to
one on this task. Moreover, we can condition on initial part of
sentence, and generate rest of it. Tables \ref{tab:a}, \ref{tab:aabb}
shows exemples of generated sequences.


\begin{table}[t]
\tiny
\centering
\begin{tabular}{l}
\hline
Generated sequence from input sequence ``a'' \\
\hline
 abbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccdd\\
 aabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccd\\
 aabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccd\\
 abbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccdd\\
 adaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbc\\
\hline
\end{tabular}
\caption{Toy example showing the memory of the RNN. See text for details.}
        \label{tab:a}
\end{table}



\begin{table}[t]
\tiny
\centering
\begin{tabular}{l}
\hline
Generated sequence from input sequence ``aabb'' \\
\hline
 aabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccd\\
 aabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccd\\
 aabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccd\\
 aabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccd\\
 aabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccddaabbccd\\
\hline
\end{tabular}
\caption{Given an initial sequence ``aabb'', the subsequent sequence
  is unambiguous.}
        \label{tab:aabb}
\end{table}



\subsection{Penn Treebank Corpus}
After 24 epochs of training (8h) we have achieved a perplexity on the
test data of 2.94. This compares favorably with
\cite{mikolov2012statistical} which reports a perplexity of 2.6 from
their best model.
Table \ref{tab:penn} presents generated sentences for our model.


\begin{table}[t]
\tiny
\centering
\begin{tabular}{l}
\hline
Generated sequence from input sequence ``My name is'' \\
\hline
My name is relatively ms. $<$unk$>$ hill which i limit it social it can \\
My name is r. $<$unk$>$ eurocom $<$unk$>$ to $<$unk$>$ more $<$unk$>$ \\
My name is $<$unk$>$ on smaller sales are clearance by long-term alterna\\
My name is reinvestment on senate is a voting oil co. his claim of s\\
My name issues culming was having a market acknowledged that it is in \\

\hline
\end{tabular}
\caption{Most of generated words are correct English words. Moreover, they
are combined in an approximately grammatical way.}
        \label{tab:penn}
\end{table}


\section{Discussion}
Although the RNN language model generates impressively realistic text,
it is still far from
human generated text. It is crucial to understand what components are
the missing to close the gap to human performance. It might be an optimization problem, or
maybe a missing computational unit problem. We would like to understand these challenges
on the large scale datasets both in the context of NLP, as well as computer vision tasks.


\section{Acknowledgment}
Thanks are due to Ilya Sutskever, Tomas Mikolov and Caglar Gulcehre
for insightful discussions on how to train RNNs.

\bibliography{bibliography}
\bibliographystyle{icml2014}

\end{document} 

